---
title: Integración Monte Carlo via cadenas de Markov
output:
  pdf_document:
    extra_dependencies: awesomebox
---

# Instrucciones:

Resuelve los siguientes ejercicios. Una entrega exitosa será la que pueda
compilar y generar un pdf sin problemas.

```{r knit, eval = FALSE, include = FALSE}
# Este bloque es sólo para que puedas verificar que tu codigo compila. Corre esta línea y después busca en tu directorio el pdf. 
rmarkdown::render("assignment.Rmd", 
                  output_file = "entrega-<usuario github>.pdf", 
                  clean = TRUE)
```

# Ejercicios 

```{r setup}
library(tidyverse)
library(scales)
library(patchwork)
library(R6)
```

1. _Muestreo por importancia_. Realiza las siguientes modificaciones al ejemplo 
de clase para **muestreo por importancia**.

- Considera una $M$ mayor y reporta lo que encuentras en experimentos numéricos.
- Considera una $M$ mas pequeña de lo necesario para que $\pi \leq M \rho$ y
reporta lo que encuentras en experimentos numéricos.

2. _El vendedor ambulante en continuo_. Considera el objetivo de generar muestras aleatorias de una variable aleatoria $\theta \sim \mathsf{N}(3,2)$. Considera propuestas del estilo
$$
\theta_\star | \theta_t \sim \mathsf{Uniforme}( \theta_t - \omega, \theta_t + \omega)\,.
$$
Realiza la generación de variables aleatorias por medio de una cadena de Markov utilizando Metropolis-Hastings considerando
$\omega = 0.01$, $\omega = 1$ y $\omega=100$. Reporta tasas de aceptación y el promedio de $\theta$ de acuerdo al modelo de la variable aleatoria. Utiliza cadenas de tamaño 10,000. 

3. _Variables aleatorias truncadas_ Utiliza el método de Metropolis-Hastings
para generar números aleatorios de una variable aleatoria Normal truncada con
función de densidad objetivo 
$$ \pi(x) \propto \exp\left( -\frac{(x - 3)^2}{32} \right) \mathbb{I}\{1 \leq x \leq 8\}\,,$$
donde $\mathbb{I}$ denota la función indicadora. Considera propuestas para una
caminata aleatoria $\mathsf{N}(x_t , 1)$ y empieza tu cadena en $x_0 =3$.
Reporta tasas de aceptación y el promedio. Utiliza cadenas de tamaño 10,000.

4. _Modelo lineal_: 

Utilizaremos los datos de `Howell1` que se presentan a continuación.


```{r}

source("./utilities/models.R")
library(rethinking)
data(Howell1)
datos <- Howell1 |> as_tibble()
datos |> print(n = 5)

```

El modelo bayesiano con el que trabajaremos es el siguiente
\begin{gather}
y_i \sim \mathsf{N}(\mu_i, \sigma) \\
\mu_i = \alpha + \beta x_i \\
\alpha \sim \mathsf{N}(0, 10)\\
\beta  \sim \mathsf{N}(0, 1)\\
\sigma \sim \mathsf{Exp}(1)\\
\end{gather}
donde $y_i$ es la estatura, $x_i$ es el peso, y asumimos que sólo es útil el modelo para personas menores a 18 años. 

- Identifica los parámetros desconocidos del modelo. 
- Utiliza el código de clase para generar muestras del modelo posterior para los parámetros que identificaste en el punto anterior. Genera $N=10,000$ simulaciones de dicho modelo. 

```{r}
previa <- PriorModel$new()

g.previa <- previa$sample(100) |> 
  as_tibble() |> 
  mutate(iter = 1:n()) |> 
  ggplot() + 
    geom_abline(aes(intercept = alpha, 
                    slope = beta)) + 
    coord_cartesian(ylim = c(-5, 5), 
                    xlim = c(-5, 5))

g.previa

g.previa <- datos |> 
  filter(age < 18) |> 
  ggplot(aes(weight, height)) + 
    geom_point(color = "white") + 
    geom_abline(data = previa$sample(100) |> as_tibble(), 
                aes(intercept = alpha, 
                    slope = beta), alpha = .1) + sin_lineas 
g.previa
```

```{r}

verosimilitud <- datos |> 
  filter(age < 18) |> 
  LikelihoodModel$new()

```

```{r}
posterior <- PosteriorModel$new(verosimilitud, previa)
```


```{r}
datos |> 
  filter(age < 18) %>%
  lm(height ~ weight, .) |> 
  summary() 
```

```{r}
muestreo <- previa 
objetivo <- verosimilitud

mcmc <- crea_metropolis_hastings_previa(objetivo, muestreo)

set.seed(108727)
samples <- mcmc(10000) |> 
  as_tibble() 
```

```{r}
samples |> 
  pivot_longer(1:4) |> 
  group_by(name) |> 
  summarise(.estimate = mean(value))
```

```{r}
g.posterior <- datos |> 
  filter(age < 18) |> 
  # mutate(height = height - mean(height), 
  #        weight = weight - mean(weight)) |> 
  ggplot(aes(weight, height)) + 
    geom_point() + 
    geom_abline(data = samples, 
                aes(intercept = alpha, 
                    slope = beta), 
                    alpha = .3) + 
                    sin_lineas

g.previa + g.posterior
```

```{r}
posterior <- PosteriorModel$new(verosimilitud, previa)
objetivo  <- posterior

mu    <- c(0, 0, 0)
omega <- .15
Sigma <- omega * diag(c(1, 1, 1))
muestreo <- ModeloNormalMultivariado$new(mu, Sigma)

mcmc <- crea_metropolis_hastings(objetivo, muestreo)

```

```{r}
set.seed(108727)
samples <- mcmc(10000) |> 
  as_tibble() 
```

```{r}
samples <- samples |> 
  mutate(.iter = 1:n()) |> 
  filter(.iter >= 3000)
```

```{r}
samples |> 
  pivot_longer(1:4) |> 
  group_by(name) |> 
  summarise(.estimate = mean(value))
```

```{r}
samples |> 
  select(-accept) |> 
  pivot_longer(1:3) |> 
  group_by(name) |> 
  ggplot(aes(value)) + 
    geom_histogram(bins = 20, color = "white") + 
    facet_wrap(~name, scales = "free") + sin_lineas
```

```{r}

samples |>  
  select(-accept) |> 
  mutate(.distribucion = "posterior") |>
  rbind(
    previa$sample(10000) |>
      as_tibble() |>
      mutate(.distribucion = "previa")
  ) |>
  pivot_longer(1:3) |> 
  ggplot(aes(value, group = .distribucion)) + 
    geom_histogram(aes(fill = .distribucion)) + 
    facet_wrap(~name, scales = "free") +   
    sin_lineas
```

```{r}
g.posterior <- datos |> 
  filter(age < 18) |> 
  # mutate(height = height - mean(height), 
  #        weight = weight - mean(weight)) |> 
  ggplot(aes(weight, height)) + 
    geom_abline(data = samples, 
                aes(intercept = alpha, 
                    slope = beta), 
                    color = "salmon",
                    alpha = .05) + 
    geom_point() + sin_lineas

g.posterior
```

```{r}
g.previa + g.posterior
```

- Utiliza el modelo bayesiano construido arriba para reportar las alturas de 4 personas a las que no se les registraron sus alturas. Provee intervalos al $87\%$.

```{r}
new_data <- tibble( weight = c(46.95, 43.72, 64.78, 32.59, 54.63))
new_data 
```

```{r}
weight <- 46.95
samples |> 
  mutate(mu = alpha + beta * weight, 
        y_tilde = rnorm(n(), mu, sd = sigma)) |> 
  summarise(.mean = mean(y_tilde), 
            .error_mc = sqrt(var(y_tilde)/n()), 
            .lim_inf  = quantile(y_tilde, (1-.83)/2), 
            .lim_sup  = quantile(y_tilde, 1 - (1-.83)/2), 
            .length   = .lim_sup - .lim_inf)
```

```{r}
calcula_predictiva <- function(datos){
  weight <- pull(datos, weight)
  samples |> 
    mutate(mu = alpha + beta * weight, 
          y_tilde = rnorm(n(), mu, sd = sigma)) |> 
    summarise(.mean = mean(y_tilde), 
              .error_mc = sqrt(var(y_tilde)/n()), 
              .lim_inf  = quantile(y_tilde, (1-.83)/2), 
              .lim_sup  = quantile(y_tilde, 1 - (1-.83)/2), 
              .length   = .lim_sup - .lim_inf)
}
```

```{r}
new_data |> 
  mutate(id = 1:5) |> 
  nest(persona = weight) |> 
  mutate(resultados_predictiva = purrr::map(persona, calcula_predictiva)) |> 
  unnest(2:3)
```
